{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Transformation and split Job!\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to Transformation and split Job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import ipytest\n",
    "\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "DATA_FILEPATH = \"../../data/penguins.csv\"\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True)\n",
    "\n",
    "# By default, The SageMaker SDK logs events related to the default\n",
    "# configuration using the INFO level. To prevent these from spoiling\n",
    "# the output of this notebook cells, we can change the logging\n",
    "# level to ERROR instead.\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.environ[\"BUCKET\"]\n",
    "role = os.environ[\"ROLE\"]\n",
    "\n",
    "COMET_API_KEY = os.environ.get(\"COMET_API_KEY\", None)\n",
    "COMET_PROJECT_NAME = os.environ.get(\"COMET_PROJECT_NAME\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# We can retrieve the architecture of the local\n",
    "# computer using the `uname -m` command.\n",
    "architecture = !(uname -m)\n",
    "\n",
    "IS_ARM64_ARCHITECTURE = architecture[0] == \"arm64\"\n",
    "print(IS_ARM64_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None\n",
    "\n",
    "if LOCAL_MODE:\n",
    "    config = {\n",
    "        \"session\": LocalPipelineSession(default_bucket=bucket),\n",
    "        \"instance_type\": \"local\",\n",
    "        # We need to use a custom Docker image when we run the pipeline\n",
    "        # in Local Model on an ARM64 machine.\n",
    "        \"image\": (\n",
    "            \"sagemaker-tensorflow-toolkit-local\" if IS_ARM64_ARCHITECTURE else None\n",
    "        ),\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": \"ml.m5.xlarge\",\n",
    "        \"image\": None,\n",
    "    }\n",
    "\n",
    "# These specific settings refer to the SageMaker\n",
    "# TensorFlow container we'll use.\n",
    "config[\"framework_version\"] = \"2.12\"\n",
    "config[\"py_version\"] = \"py310\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/penguins\"\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Stratified sampling\n",
    "\n",
    "Modify the preprocessing script to split the dataset using stratified sampling instead of random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_FOLDER code\n"
     ]
    }
   ],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/processing\"])\n",
    "print(\"CODE_FOLDER\",CODE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo for writefile linemagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/processing/sample_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/sample_hello.py\n",
    "print(\"Hello Jupyter!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/processing/stratified_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/stratified_script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])],\n",
    "    )\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder(),\n",
    "    )\n",
    "\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\",\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "            (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    _save_train_baseline(base_directory, df_train)\n",
    "    _save_test_baseline(base_directory, df_test)\n",
    "\n",
    "    y_train = target_transformer.fit_transform(\n",
    "        np.array(df_train.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_validation = target_transformer.transform(\n",
    "        np.array(df_validation.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_test = target_transformer.transform(\n",
    "        np.array(df_test.species.values).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "    df_train = df_train.drop(\"species\", axis=1)\n",
    "    df_validation = df_validation.drop(\"species\", axis=1)\n",
    "    df_test = df_test.drop(\"species\", axis=1)\n",
    "\n",
    "    X_train = features_transformer.fit_transform(df_train)  # noqa: N806\n",
    "    X_validation = features_transformer.transform(df_validation)  # noqa: N806\n",
    "    X_test = features_transformer.transform(df_test)  # noqa: N806\n",
    "\n",
    "    _save_splits(\n",
    "        base_directory,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df):\n",
    "    \"\"\"Split the data into train, validation, and test.\"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3, stratify = df['species'])\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5, stratify = temp['species'])\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_train_baseline(base_directory, df_train):\n",
    "    \"\"\"Save the untransformed training data to disk.\n",
    "\n",
    "    We will need the training data to compute a baseline to\n",
    "    determine the quality of the data that the model receives\n",
    "    when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"train-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_train.copy().dropna()\n",
    "\n",
    "    # To compute the data quality baseline, we don't need the\n",
    "    # target variable, so we'll drop it from the dataframe.\n",
    "    df = df.drop(\"species\", axis=1)\n",
    "\n",
    "    df.to_csv(baseline_path / \"train-baseline.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "def _save_test_baseline(base_directory, df_test):\n",
    "    \"\"\"Save the untransformed test data to disk.\n",
    "\n",
    "    We will need the test data to compute a baseline to\n",
    "    determine the quality of the model predictions when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"test-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_test.copy().dropna()\n",
    "\n",
    "    # We'll use the test baseline to generate predictions later,\n",
    "    # and we can't have a header line because the model won't be\n",
    "    # able to make a prediction for it.\n",
    "    df.to_csv(baseline_path / \"test-baseline.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_validation,  # noqa: N803\n",
    "    y_validation,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\",\n",
    "        header=False,\n",
    "        index=False,\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory, target_transformer, features_transformer):\n",
    "    \"\"\"Save the Scikit-Learn transformation pipelines.\n",
    "\n",
    "    This function creates a model.tar.gz file that\n",
    "    contains the two transformation pipelines we built\n",
    "    to transform the data.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, Path(directory) / \"target.joblib\")\n",
    "        joblib.dump(features_transformer, Path(directory) / \"features.joblib\")\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{(model_path / 'model.tar.gz').as_posix()}\", \"w:gz\") as tar:\n",
    "            tar.add(Path(directory) / \"target.joblib\", arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                Path(directory) / \"features.joblib\", arcname=\"features.joblib\",\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.96s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "# | code-fold: true\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('code')\n",
    "import processing\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pytest\n",
    "from processing.stratified_script import preprocess\n",
    "\n",
    "\n",
    "@pytest.fixture(autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "    preprocess(base_directory=directory)\n",
    "\n",
    "    yield directory\n",
    "\n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_preprocess_generates_data_splits(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train\" in output_directories\n",
    "    assert \"validation\" in output_directories\n",
    "    assert \"test\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_generates_baselines(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train-baseline\" in output_directories\n",
    "    assert \"test-baseline\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_creates_two_models(directory):\n",
    "    model_path = directory / \"model\"\n",
    "    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n",
    "\n",
    "    assert \"features.joblib\" in tar.getnames()\n",
    "    assert \"target.joblib\" in tar.getnames()\n",
    "\n",
    "\n",
    "def test_splits_are_transformed(directory):\n",
    "    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n",
    "    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n",
    "    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n",
    "\n",
    "    # After transforming the data, the number of features should be 7:\n",
    "    # * 3 - island (one-hot encoded)\n",
    "    # * 1 - culmen_length_mm = 1\n",
    "    # * 1 - culmen_depth_mm\n",
    "    # * 1 - flipper_length_mm\n",
    "    # * 1 - body_mass_g\n",
    "    number_of_features = 7\n",
    "\n",
    "    # The transformed splits should have an additional column for the target\n",
    "    # variable.\n",
    "    assert train.shape[1] == number_of_features + 1\n",
    "    assert validation.shape[1] == number_of_features + 1\n",
    "    assert test.shape[1] == number_of_features + 1\n",
    "\n",
    "\n",
    "def test_train_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"train-baseline\" / \"train-baseline.csv\",\n",
    "        header=None,\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 0].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_test_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"test-baseline\" / \"test-baseline.csv\", header=None\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 1].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_train_baseline_includes_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\")\n",
    "    assert baseline.columns[0] == \"island\"\n",
    "\n",
    "\n",
    "def test_test_baseline_does_not_include_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\")\n",
    "    assert baseline.columns[0] != \"island\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching in sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://beercafe-ml-bucket/penguins\n"
     ]
    }
   ],
   "source": [
    "print(S3_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n",
    "    # If you haven't requested a quota increase yet, you can use an\n",
    "    # `ml.t3.medium` instance type instead. This will work out of the box, but\n",
    "    # the Processing Job will take significantly longer than it should have.\n",
    "    # To get access to `ml.m5.xlarge` instances, you can request a quota\n",
    "    # increase under the Service Quotas section in your AWS account.\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hemch\\git\\ml-yard\\.venv\\lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(CODE_FOLDER / 'processing' / 'stratified_script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train_stratified\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\",\n",
    "                source=\"/opt/ml/processing/validation\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/validation_stratified\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test_stratified\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"model\",\n",
    "                source=\"/opt/ml/processing/model\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/model_stratified\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train-baseline\",\n",
    "                source=\"/opt/ml/processing/train-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train-baseline-stratified\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test-baseline\",\n",
    "                source=\"/opt/ml/processing/test-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test-baseline-stratified\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:767398043272:pipeline/stratified-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'd3a20aa2-8ed6-4c9c-9cfd-c87314d8454e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd3a20aa2-8ed6-4c9c-9cfd-c87314d8454e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '87',\n",
       "   'date': 'Wed, 17 Apr 2024 16:05:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "stratified_pipeline = Pipeline(\n",
    "    name=\"stratified-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"]\n",
    ")\n",
    "\n",
    "stratified_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:767398043272:pipeline/stratified-pipeline/execution/4p6hr1olhdcx', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x00000233B10921A0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%script false --no-raise-error\n",
    "# | eval: false\n",
    "\n",
    "stratified_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE model/\n",
      "                           PRE model_stratified/\n",
      "                           PRE test-baseline-stratified/\n",
      "                           PRE test-baseline/\n",
      "                           PRE test/\n",
      "                           PRE test_stratified/\n",
      "                           PRE train-baseline-stratified/\n",
      "                           PRE train-baseline/\n",
      "                           PRE train/\n",
      "                           PRE train_stratified/\n",
      "                           PRE validation/\n",
      "                           PRE validation_stratified/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls beercafe-ml-bucket/penguins/preprocessing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Generate random Penguins\n",
    "\n",
    "Use ChatGPT to generate a dataset with 500 random penguins and store the file in S3. Run the pipeline pointing the dataset_location parameter to the new dataset. By overriding default parameters during a pipeline execution, you can process different datasets without having to modify your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Script to generate random Penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'island' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Generate random data for penguins\u001b[39;00m\n\u001b[0;32m     17\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# for reproducibility\u001b[39;00m\n\u001b[0;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(species, p\u001b[38;5;241m=\u001b[39mspecies_probabilities[\u001b[43misland\u001b[49m], size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misland\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(islands, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mculmen_length_mm\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mculmen_depth_mm\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m17\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflipper_length_mm\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody_mass_g\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfemale\u001b[39m\u001b[38;5;124m'\u001b[39m], size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m     26\u001b[0m }\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m penguins_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'island' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the islands and their corresponding species\n",
    "islands = ['Saikhowa', 'Majuli', 'Umananda']\n",
    "species = ['Adelie', 'Gentoo', 'Chinstrap']\n",
    "\n",
    "# Define probabilities for each species on each island\n",
    "# These probabilities should sum up to 1 for each island\n",
    "species_probabilities = {\n",
    "    'Saikhowa': [0.6, 0.2, 0.2],\n",
    "    'Majuli': [0.1, 0.8, 0.1],\n",
    "    'Umananda': [0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Generate random data for penguins\n",
    "np.random.seed(42)  # for reproducibility\n",
    "data = {\n",
    "    'species': np.random.choice(species, p=species_probabilities[island], size=500),\n",
    "    'island': np.random.choice(islands, size=500),\n",
    "    'culmen_length_mm': np.random.normal(loc=40, scale=5, size=500),\n",
    "    'culmen_depth_mm': np.random.normal(loc=17, scale=2, size=500),\n",
    "    'flipper_length_mm': np.random.normal(loc=200, scale=10, size=500),\n",
    "    'body_mass_g': np.random.normal(loc=4000, scale=500, size=500),\n",
    "    'sex': np.random.choice(['male', 'female'], size=500)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "penguins_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(penguins_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
