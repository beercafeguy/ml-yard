{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Transformation and split Job!\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to Transformation and split Job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import ipytest\n",
    "\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "DATA_FILEPATH = \"../data/penguins.csv\"\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True)\n",
    "\n",
    "# By default, The SageMaker SDK logs events related to the default\n",
    "# configuration using the INFO level. To prevent these from spoiling\n",
    "# the output of this notebook cells, we can change the logging\n",
    "# level to ERROR instead.\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.environ[\"BUCKET\"]\n",
    "role = os.environ[\"ROLE\"]\n",
    "\n",
    "COMET_API_KEY = os.environ.get(\"COMET_API_KEY\", None)\n",
    "COMET_PROJECT_NAME = os.environ.get(\"COMET_PROJECT_NAME\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# We can retrieve the architecture of the local\n",
    "# computer using the `uname -m` command.\n",
    "architecture = !(uname -m)\n",
    "\n",
    "IS_ARM64_ARCHITECTURE = architecture[0] == \"arm64\"\n",
    "print(IS_ARM64_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Windows Support for Local Mode is Experimental\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None\n",
    "\n",
    "if LOCAL_MODE:\n",
    "    config = {\n",
    "        \"session\": LocalPipelineSession(default_bucket=bucket),\n",
    "        \"instance_type\": \"local\",\n",
    "        # We need to use a custom Docker image when we run the pipeline\n",
    "        # in Local Model on an ARM64 machine.\n",
    "        \"image\": (\n",
    "            \"sagemaker-tensorflow-toolkit-local\" if IS_ARM64_ARCHITECTURE else None\n",
    "        ),\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": \"ml.m5.xlarge\",\n",
    "        \"image\": None,\n",
    "    }\n",
    "\n",
    "# These specific settings refer to the SageMaker\n",
    "# TensorFlow container we'll use.\n",
    "config[\"framework_version\"] = \"2.12\"\n",
    "config[\"py_version\"] = \"py310\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/penguins\"\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_FOLDER code\n"
     ]
    }
   ],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/processing\"])\n",
    "print(\"CODE_FOLDER\",CODE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo for writefile linemagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/processing/hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/hello.py\n",
    "print(\"Hello Jupyter!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/processing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/script.py\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "\n",
    "    \"\"\"\n",
    "    This Function loads the supplied data, splits and transforms the same\n",
    "    \"\"\"\n",
    "\n",
    "    df = _read_data_from_csv_files(base_directory)\n",
    "\n",
    "    # This is to apply label encoding on target variable\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transfomers = [(\"species\", OrdinalEncoer(),[0])]\n",
    "    )\n",
    "\n",
    "    # This is to transform numerical columns\n",
    "    # This first imputes missing values with mean of the feature and then applies StandardScaler\n",
    "    numerical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler()\n",
    "    )\n",
    "\n",
    "    # This is to transform categorical columns into OHE columns\n",
    "    # Before OHE, we impute missing values with most frequent value\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder()\n",
    "    )\n",
    "\n",
    "    # note here we are not applying categorical transformations for SEX column\n",
    "    # This is because we don't think SEX feature in this data has any predictive power\n",
    "    # o \n",
    "    feature_transformers = ColumnTransformer(\n",
    "        transformers = [\n",
    "            (\n",
    "                \"numeric\",\n",
    "                numerical_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\")\n",
    "            ),\n",
    "            (\n",
    "                \"categorical\",\n",
    "                categorical_transformer,\n",
    "                ['island']\n",
    "            ) \n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def _read_data_from_csv_files(base_directory):\n",
    "    \"\"\"\n",
    "    This function reads every single available csv files and puts those in one DF\n",
    "    \"\"\"\n",
    "\n",
    "    input_dir = Path(base_directory) / \"input\"\n",
    "    files = [file for file in input_dir.glob(\"*.csv\")]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(f\"There are no CSV files in {input_dir.as_posix()}\")\n",
    "    \n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    return df.sample(frac=1, random_state=2024) # This is to shuffle the data\n",
    "\n",
    "if __name__ = \"__main__\":\n",
    "    preprocess(base_directory = \"/opt/ml/preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
